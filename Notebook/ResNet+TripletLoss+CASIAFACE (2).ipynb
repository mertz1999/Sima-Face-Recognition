{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rvwhhKu09yf"
      },
      "source": [
        "## Load Python Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtlUChqXFyKd"
      },
      "outputs": [],
      "source": [
        "# --- load packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.modules.distance import PairwiseDistance\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import os\n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OczsCHs-1KlQ"
      },
      "source": [
        "## Set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmZ7_4FNRvDG"
      },
      "outputs": [],
      "source": [
        "# --- Set all Parameters\n",
        "DatasetFolder = \"./CASIA-WebFace\"  # path to Dataset folder\n",
        "ResNet_sel = \"18\"                  # select ResNet type\n",
        "NumberID = 10575                   # Number of ID in dataset \n",
        "batch_size = 256                   # size of batch size\n",
        "Triplet_size = 10000 * batch_size  # size of total Triplets\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "loss_margin = 0.6                  # Margin for Triplet loss\n",
        "learning_rate = 0.075              # choose Learning Rate(note that this value will be change during training)\n",
        "epochs = 200                       # number of iteration over total dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d87k5QG91Pxn"
      },
      "source": [
        "## Download Datasets\n",
        "#### In this section we download CASIA-WebFace and LFW-Dataset\n",
        "#### we use CAISA-WebFace for Training and LFW for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzFRzcjyRKHL"
      },
      "outputs": [],
      "source": [
        "# --- Download CASIA-WebFace Dataset\n",
        "print(40*\"=\" + \" Download CASIA WebFace \" + 40*'=')\n",
        "! gdown --id 1Of_EVz-yHV7QVWQGihYfvtny9Ne8qXVz\n",
        "! unzip CASIA-WebFace.zip\n",
        "! rm CASIA-WebFace.zip\n",
        "\n",
        "# --- Download LFW Dataset\n",
        "print(40*\"=\" + \" Download LFW \" + 40*'=')\n",
        "! wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n",
        "! tar -xvzf lfw-deepfunneled.tgz\n",
        "! rm lfw-deepfunneled.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0g7PWjD1pe5"
      },
      "source": [
        "# Define ResNet Parts\n",
        "#### 1. Residual block\n",
        "#### 2. Make ResNet by Prv. block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKAiYM-HLkqn"
      },
      "outputs": [],
      "source": [
        "# --- Residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, downsample=1):\n",
        "    super().__init__()\n",
        "    # --- Variables\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.downsample = downsample\n",
        "\n",
        "    # --- Residual parts\n",
        "    # --- Conv part\n",
        "    self.blocks = nn.Sequential(OrderedDict(\n",
        "        {\n",
        "            # --- First Conv\n",
        "            'conv1' : nn.Conv2d(self.in_channels, self.out_channels, kernel_size=3, stride=self.downsample, padding=1, bias=False),\n",
        "            'bn1'   : nn.BatchNorm2d(self.out_channels),\n",
        "            'Relu1' : nn.ReLU(),\n",
        "         \n",
        "            # --- Secound Conv\n",
        "            'conv2' : nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            'bn2'   : nn.BatchNorm2d(self.out_channels)\n",
        "        }\n",
        "    ))\n",
        "    # --- shortcut part\n",
        "    self.shortcut = nn.Sequential(OrderedDict(\n",
        "        {\n",
        "            'conv' : nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1, stride=self.downsample, bias=False),\n",
        "            'bn'   : nn.BatchNorm2d(self.out_channels)\n",
        "        }\n",
        "    ))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    if (self.in_channels != self.out_channels) : residual = self.shortcut(x)\n",
        "    x = self.blocks(x)\n",
        "    x += residual\n",
        "    return x\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZmtQM6cTL_Z"
      },
      "outputs": [],
      "source": [
        "# # --- Test Residual block\n",
        "# dummy = torch.ones((1, 32, 140, 140))\n",
        "\n",
        "# block = ResidualBlock(32, 64)\n",
        "# block(dummy).shape\n",
        "# print(block)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqisIePXTSRx"
      },
      "outputs": [],
      "source": [
        "# --- Make ResNet18\n",
        "class ResNet18(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # --- Pre layers with 7*7 conv with stride2 and a max-pooling\n",
        "    self.PreBlocks = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, kernel_size=7, padding=3, stride=2, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    )\n",
        "\n",
        "    # --- Define all Residual Blocks here\n",
        "    self.CoreBlocka = nn.Sequential(\n",
        "        ResidualBlock(64,64 ,downsample=1),\n",
        "        ResidualBlock(64,64 ,downsample=1),\n",
        "        # ResidualBlock(64,64 ,downsample=1),\n",
        "\n",
        "        ResidualBlock(64,128 ,downsample=2),\n",
        "        ResidualBlock(128,128 ,downsample=1),\n",
        "        # ResidualBlock(128,128 ,downsample=1),\n",
        "        # ResidualBlock(128,128 ,downsample=1),\n",
        "\n",
        "        ResidualBlock(128,256 ,downsample=2),\n",
        "        ResidualBlock(256,256 ,downsample=1),\n",
        "        # ResidualBlock(256,256 ,downsample=1),\n",
        "        # ResidualBlock(256,256 ,downsample=1),\n",
        "        # ResidualBlock(256,256 ,downsample=1),\n",
        "        # ResidualBlock(256,256 ,downsample=1),\n",
        "\n",
        "        ResidualBlock(256,512 ,downsample=2),\n",
        "        ResidualBlock(512,512 ,downsample=1),\n",
        "        # ResidualBlock(512,512 ,downsample=1)\n",
        "    )\n",
        "\n",
        "    # --- Make Average pooling\n",
        "    self.avg = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    # --- FC layer for output\n",
        "    self.fc = nn.Linear(512, 512, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.PreBlocks(x)\n",
        "    x = self.CoreBlocka(x)\n",
        "    x = self.avg(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "    x = F.normalize(x, p=2, dim=1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w28hiEFaTxrB"
      },
      "outputs": [],
      "source": [
        "# dummy = torch.ones((1, 3, 114, 144))\n",
        "model = ResNet18()\n",
        "# model\n",
        "# res = model(dummy)\n",
        "\n",
        "model.to(device)\n",
        "summary(model, (3, 114, 114))\n",
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-pLtZni15ru"
      },
      "source": [
        "# Make TripletLoss Class "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chdPWwbpTxtO"
      },
      "outputs": [],
      "source": [
        "# --- Triplet loss\n",
        "\"\"\"\n",
        "    This code was imported from tbmoon's 'facenet' repository:\n",
        "    https://github.com/tbmoon/facenet/blob/master/utils.py\n",
        "    \n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "from torch.nn.modules.distance import PairwiseDistance\n",
        "\n",
        "\n",
        "class TripletLoss(Function):\n",
        "    def __init__(self, margin):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.pdist = PairwiseDistance(p=2)\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        pos_dist = self.pdist.forward(anchor, positive)\n",
        "        neg_dist = self.pdist.forward(anchor, negative)\n",
        "        hinge_dist = torch.clamp(self.margin + pos_dist - neg_dist, min=0.0)\n",
        "        loss = torch.mean(hinge_dist)\n",
        "        # print(torch.mean(pos_dist).item(), torch.mean(neg_dist).item(), loss.item())\n",
        "        # print(\"pos_dist\", pos_dist)\n",
        "        # print(\"neg_dist\", neg_dist)\n",
        "        # print(self.margin + pos_dist - neg_dist)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jb4saFVP--_"
      },
      "source": [
        "# Make Triplet Dataset from CASIA-WebFace\n",
        "\n",
        "##### 1. Make Triplet pairs\n",
        "##### 2. Make them zip\n",
        "##### 3. Make Dataset Calss\n",
        "##### 4. Define Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhvoWAWATxvs"
      },
      "outputs": [],
      "source": [
        "# --- Create Triplet Datasets ---\n",
        "# --- make a list of ids and folders\n",
        "selected_ids = np.uint32(np.round((np.random.rand(int(Triplet_size))) * (NumberID-1)))\n",
        "folders = os.listdir(\"./CASIA-WebFace/\")\n",
        "\n",
        "# --- Itrate on each id and make Triplets list\n",
        "TripletList = []\n",
        "\n",
        "for index,id in enumerate(selected_ids):\n",
        "\n",
        "  # --- find name of id faces folder\n",
        "  id_str = str(folders[id])\n",
        "\n",
        "  # --- find list of faces in this folder\n",
        "  number_faces = os.listdir(\"./CASIA-WebFace/\"+id_str)\n",
        "\n",
        "  # --- Get two Random number for Anchor and Positive\n",
        "  while(True):\n",
        "    two_random = np.uint32(np.round(np.random.rand(2) * (len(number_faces)-1))) \n",
        "    if (two_random[0] != two_random[1]):\n",
        "      break\n",
        "\n",
        "  # --- Make Anchor and Positive image\n",
        "  Anchor   = str(number_faces[two_random[0]])\n",
        "  Positive = str(number_faces[two_random[1]])\n",
        "\n",
        "  # --- Make Negative image\n",
        "  while(True):\n",
        "    neg_id = np.uint32(np.round(np.random.rand(1) * (NumberID-1)))\n",
        "    if (neg_id != id):\n",
        "      break\n",
        "  # --- number of images in negative Folder\n",
        "  neg_id_str   = str(folders[neg_id[0]])\n",
        "  number_faces = os.listdir(\"./CASIA-WebFace/\"+neg_id_str)\n",
        "  one_random   = np.uint32(np.round(np.random.rand(1) * (len(number_faces)-1))) \n",
        "  Negative     = str(number_faces[one_random[0]])\n",
        "  \n",
        "  # --- insert Anchor, Positive and Negative image path to TripletList\n",
        "  TempList = [\"\",\"\",\"\"]\n",
        "  TempList[0] =  id_str + \"/\" + Anchor\n",
        "  TempList[1] =  id_str + \"/\" + Positive\n",
        "  TempList[2] =  neg_id_str + \"/\" + Negative\n",
        "  TripletList.append(TempList)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjQxSAlyTxx1"
      },
      "outputs": [],
      "source": [
        "# # --- Make dataset Triplets File\n",
        "# f = open(\"CASIA-WebFace-Triplets.txt\", \"w\")\n",
        "# for index, triplet in enumerate(TripletList):\n",
        "#   f.write(triplet[0] + \" \" + triplet[1] + \" \" + triplet[2])\n",
        "#   if (index != len(TripletList)-1):\n",
        "#     f.write(\"\\n\")\n",
        "# f.close()\n",
        "\n",
        "\n",
        "# # --- Make zipFile if you need\n",
        "# !zip -r CASIA-WebFace-Triplets.zip CASIA-WebFace-Triplets.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEFq-WKhtWSw"
      },
      "outputs": [],
      "source": [
        "# # --- Read zip File and extract TripletList\n",
        "# TripletList = []\n",
        "# # !unzip CASIA-WebFace-Triplets.zip\n",
        "\n",
        "# # --- Read text file\n",
        "# with open('CASIA-WebFace-Triplets.txt') as f:\n",
        "#     lines = f.readlines()\n",
        "#     for line in lines:\n",
        "#       TripletList.append(line.split(' '))\n",
        "#       TripletList[-1][2] = TripletList[-1][2][0:-1]\n",
        "\n",
        "# # --- Print some data\n",
        "# print(TripletList[0:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHzvdd4MTx1P"
      },
      "outputs": [],
      "source": [
        "# --- Make Pytorch Dataset Class for Triplets \n",
        "class TripletFaceDatset(Dataset):\n",
        "  def __init__(self, list_of_triplets, transform=None):\n",
        "    # --- initializing values\n",
        "    print(\"Start Creating Triplets Dataset from CASIA-WebFace\")\n",
        "    self.list_of_triplets = list_of_triplets\n",
        "    self.transform = transform\n",
        "\n",
        "  # --- getitem function\n",
        "  def __getitem__(self, index):\n",
        "    # --- get images path and read faces\n",
        "    anc_img_path, pos_img_path, neg_img_path = self.list_of_triplets[index]\n",
        "    anc_img = cv2.imread('./CASIA-WebFace/'+anc_img_path)\n",
        "    pos_img = cv2.imread('./CASIA-WebFace/'+pos_img_path)\n",
        "    neg_img = cv2.imread('./CASIA-WebFace/'+neg_img_path)\n",
        "\n",
        "    # anc_img = cv2.resize(anc_img, (114,114))\n",
        "    # pos_img = cv2.resize(pos_img, (114,114))\n",
        "    # neg_img = cv2.resize(neg_img, (114,114))\n",
        "\n",
        "\n",
        "    # --- set transform\n",
        "    if self.transform:\n",
        "      anc_img = self.transform(anc_img)\n",
        "      pos_img = self.transform(pos_img)\n",
        "      neg_img = self.transform(neg_img)\n",
        "    \n",
        "    return {'anc_img' : anc_img,\n",
        "            'pos_img' : pos_img,\n",
        "            'neg_img' : neg_img}\n",
        "  \n",
        "  # --- return len of triplets\n",
        "  def __len__(self):\n",
        "    return len(self.list_of_triplets)\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpCnypG778KL"
      },
      "outputs": [],
      "source": [
        "# --- Define Transforms\n",
        "transform_list =transforms.Compose([\n",
        "                      transforms.ToPILImage(),\n",
        "                      transforms.Resize((140,140)),\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                           std =[0.229, 0.224, 0.225])\n",
        "                 ])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hufkOT7yi_BJ",
        "outputId": "29337841-7c7f-4ab8-f449-6d7a592fc953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Creating Triplets Dataset from CASIA-WebFace\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 140, 140])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# --- Test Dataset\n",
        "triplet_dataset = TripletFaceDatset(TripletList, transform_list)\n",
        "triplet_dataset[0]['anc_img'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJo0ZmpSQxRt"
      },
      "source": [
        "# LFW Evaluation\n",
        "##### 1. Face detection function\n",
        "##### 2. Load LFW Pairs .npy file\n",
        "##### 3. Define Function for evaluation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPh81E-yE8vy"
      },
      "outputs": [],
      "source": [
        "# -------------------------- UTILS CELL -------------------------------\n",
        "\n",
        "trained_face_data = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
        "# --- define Functions\n",
        "def face_detect(file_name):\n",
        "  flag = True\n",
        "  # Choose an image to detect faces in\n",
        "  img = cv2.imread(file_name)\n",
        "\n",
        "  # Must convert to greyscale\n",
        "  # grayscaled_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Detect Faces \n",
        "  # face_coordinates = trained_face_data.detectMultiScale(grayscaled_img)\n",
        "\n",
        "  # img_crop = []\n",
        "  # Draw rectangles around the faces\n",
        "  # for (x, y, w, h) in face_coordinates:\n",
        "  #   img_crop.append(img[y-20:y+h+20, x-20:x+w+20])\n",
        "\n",
        "  # --- select only Biggest\n",
        "  # big_id = 0\n",
        "  # if len(img_crop) > 1:\n",
        "  #   temp = 0\n",
        "  #   for idx, img in enumerate(img_crop):\n",
        "  #     if img.shape[0] > temp:\n",
        "  #       temp = img.shape[0]\n",
        "  #       big_id = idx\n",
        "  # elif len(img_crop) == 0:\n",
        "  #   flag = False\n",
        "  #   img_crop = [0]\n",
        "\n",
        "  # return image crop\n",
        "  # return [img_crop[big_id]], flag\n",
        "  return [img], flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nDXMJZdkUj3"
      },
      "outputs": [],
      "source": [
        "# --- LFW Dataset loading for test part\n",
        "l2_dist = PairwiseDistance(2)\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "# --- 1. Load .npy pairs path\n",
        "lfw_pairs_path = np.load('lfw_pairs_path.npy', allow_pickle=True)\n",
        "pairs_dist_list_mat = []\n",
        "pairs_dist_list_unmat = []\n",
        "valid_thresh = 0.96\n",
        "\n",
        "def lfw_validation(model):\n",
        "  global valid_thresh\n",
        "  tot_len = len(lfw_pairs_path)\n",
        "\n",
        "  model.eval() # use model in evaluation mode\n",
        "  with torch.no_grad():\n",
        "    true_match = 0\n",
        "    for path in lfw_pairs_path:\n",
        "      # --- extracting\n",
        "      pair_one_path = path['pair_one']\n",
        "      # print(pair_one_path)\n",
        "      pair_two_path = path['pair_two']\n",
        "      # print(pair_two_path)\n",
        "      matched       = int(path['matched'])\n",
        "\n",
        "      # --- detect face and resize it\n",
        "      pair_one_img, flag_one = face_detect(pair_one_path)\n",
        "      pair_two_img, flag_two = face_detect(pair_two_path)\n",
        "\n",
        "      if (flag_one==False) or (flag_two==False):\n",
        "        tot_len = tot_len-1\n",
        "        continue\n",
        "      \n",
        "\n",
        "      # --- Model Predict\n",
        "      pair_one_img = transform_list(pair_one_img[0])\n",
        "      pair_two_img = transform_list(pair_two_img[0])\n",
        "      pair_one_embed = model(torch.unsqueeze(pair_one_img, 0).to(device))\n",
        "      pair_two_embed = model(torch.unsqueeze(pair_two_img, 0).to(device))\n",
        "      \n",
        "      # print(pair_one_embed.shape)\n",
        "      # break\n",
        "      # print(pair_one_img)\n",
        "      # break\n",
        "      # --- find Distance\n",
        "      pairs_dist = l2_dist.forward(pair_one_embed, pair_two_embed)\n",
        "      if matched == 1: pairs_dist_list_mat.append(pairs_dist.item())\n",
        "      if matched == 0: pairs_dist_list_unmat.append(pairs_dist.item())\n",
        "      \n",
        "\n",
        "      # --- thrsholding\n",
        "      if (matched==1 and pairs_dist.item() <= valid_thresh) or (matched==0 and pairs_dist.item() > valid_thresh):\n",
        "        true_match += 1\n",
        "\n",
        "  valid_thresh = (np.percentile(pairs_dist_list_unmat,25) + np.percentile(pairs_dist_list_mat,75)) /2\n",
        "  print(\"Thresh :\", valid_thresh)\n",
        "  return (true_match/tot_len)*100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDPjawj6mBEk"
      },
      "outputs": [],
      "source": [
        "# img, _ = face_detect(\"./lfw-deepfunneled/Steve_Lavin/Steve_Lavin_0002.jpg\")\n",
        "# plt.imshow(img[0])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "AnsdBfyKORsi",
        "outputId": "cc8c8969-85db-4711-a8fc-c12d9f77a211"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3ea7d34704a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mvalid_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlfw_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-5984e9d2b1b2>\u001b[0m in \u001b[0;36mlfw_validation\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;31m# --- Model Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mpair_one_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_one_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       \u001b[0mpair_two_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_two_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mpair_one_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_one_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pic should be Tensor or ndarray. Got {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'NoneType'>."
          ]
        }
      ],
      "source": [
        "temp = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "for i in temp:\n",
        "  valid_thresh = i\n",
        "  print(lfw_validation(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouZDYnfmLdEI"
      },
      "outputs": [],
      "source": [
        "(np.mean(pairs_dist_list_mat) + np.mean(pairs_dist_list_unmat) )/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AysIoYDVLxAP"
      },
      "outputs": [],
      "source": [
        "ppairs_dist_list_unmat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKpJGtHhy46C"
      },
      "outputs": [],
      "source": [
        "# --- find best thresh\n",
        "round_unmat = pairs_dist_list_unmat\n",
        "round_mat = pairs_dist_list_mat\n",
        "\n",
        "print(\"----- Unmatched statistical information -----\")\n",
        "print(\"len  : \",len(round_unmat))\n",
        "print(\"min  : \", np.min(round_unmat))\n",
        "print(\"Q1   : \", np.percentile(round_unmat, 15))\n",
        "print(\"mean : \", np.mean(round_unmat))\n",
        "print(\"Q3   : \", np.percentile(round_unmat, 75))\n",
        "print(\"max  : \", np.max(round_unmat))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"----- matched statistical information -----\")\n",
        "print(\"len  : \",len(round_mat))\n",
        "print(\"min  : \", np.min(round_mat))\n",
        "print(\"Q1   : \", np.percentile(round_mat, 25))\n",
        "print(\"mean : \", np.mean(round_mat))\n",
        "print(\"Q3   : \", np.percentile(round_mat, 85))\n",
        "print(\"max  : \", np.max(round_mat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy3ZxwGoVpp4"
      },
      "source": [
        "## How to make Training Faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2BgIxYaErf8"
      },
      "outputs": [],
      "source": [
        "# Make Trianing Faster in Pytorch(Cuda):\n",
        "# 1. use number of worker\n",
        "# 2. set pin_memory\n",
        "# 3. Enable cuDNN for optimizing Conv\n",
        "# 4. using AMP\n",
        "# 5. set bias=False in conv layer if you set batch normalizing in model\n",
        "\n",
        "\n",
        "\n",
        "# source: https://betterprogramming.pub/how-to-make-your-pytorch-code-run-faster-93079f3c1f7b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpIyf3wIVvrV"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgoZ9MsYi_Dm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a55ef7-7c76-4953-bca6-a30df9e3af97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "# --- DataLoader\n",
        "face_data = torch.utils.data.DataLoader(triplet_dataset, \n",
        "                                       batch_size= batch_size,\n",
        "                                       shuffle=True,\n",
        "                                       num_workers=4,\n",
        "                                       pin_memory= True)\n",
        "\n",
        "# --- Enable cuDNN\n",
        "torch.backends.cudnn.benchmark = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1wdSL0FV2Sz"
      },
      "source": [
        "# Save Model (best acc. and last acc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkBFLWrIE3cR",
        "outputId": "45d921b0-c083-4cba-f5ef-6339acd6a1c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# --- saving model for best and last model\n",
        "\n",
        "# --- Connect to google Drive for saving models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# --- some variable for saving models\n",
        "BEST_MODEL_PATH = \"./gdrive/MyDrive/best_trained.pth\"\n",
        "LAST_MODEL_PATH = \"./gdrive/MyDrive/last_trained.pth\"\n",
        "\n",
        "\n",
        "def save_model(model_sv, loss_sv, epoch_sv, optimizer_state_sv, accuracy, accu_sv_list, loss_sv_list):\n",
        "  # --- Inputs:\n",
        "  #     1. model_sv           : orginal model that trained\n",
        "  #     2. loss_sv            : current loss\n",
        "  #     3. epoch_sv           : current epoch\n",
        "  #     4. optimizer_state_sv : current value of optimizer \n",
        "  #     5. accuracy           : current accuracy\n",
        "  \n",
        "  # --- save last epoch\n",
        "  if accuracy >= max(accu_sv_list): \n",
        "    torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "  \n",
        "  # --- save this model for checkpoint\n",
        "  torch.save({\n",
        "            'epoch': epoch_sv,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer_state_sv.state_dict(),\n",
        "            'loss': loss_sv,\n",
        "            'accu_sv_list': accu_sv_list,\n",
        "            'loss_sv_list' : loss_sv_list\n",
        "             }, LAST_MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fWrVXgxV1Da"
      },
      "source": [
        "# Load prev. model for continue training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnrcK7aUV1SJ"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- training initialize and start\n",
        "model       = ResNet18().to(device)                                                 # load model                                                       \n",
        "tiplet_loss = TripletLoss(loss_margin)                                              # load Tripletloss\n",
        "optimizer   = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
        "                         lr=learning_rate)                                          # load optimizer\n",
        "l2_dist     = PairwiseDistance(2)                                                   # L2 distance loading                                                                    # save loss values\n",
        "epoch_check = 0\n",
        "valid_arr   = []\n",
        "loss_arr    = []\n",
        "\n",
        "\n",
        "load_last_epoch = True\n",
        "if (load_last_epoch == True):\n",
        "  # --- load last model\n",
        "  # define model objects before this\n",
        "  checkpoint = torch.load(LAST_MODEL_PATH, map_location=device)                      # load model path\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])         # load state dict\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict']) # load optimizer\n",
        "  epoch_check = checkpoint['epoch']                             # load epoch\n",
        "  loss = checkpoint['loss']                                     # load loss value\n",
        "  valid_arr = checkpoint['accu_sv_list']                        # load Acc. values\n",
        "  loss_arr = checkpoint['loss_sv_list']                         # load loss values\n",
        "model.train()  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMjH-c_OLHcw",
        "outputId": "1fa2682c-f634-4e0e-b690-baf09194c3c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "epoch_check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEx1p65mxCss",
        "outputId": "ab0043b0-ce76-44b0-ac91-b003c2c39cff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.004417351509642321"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDRwR6N-gNOW"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3whW-_YfF1c",
        "outputId": "6bb355b4-22ab-45a0-95b3-300e94db5be5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/200]  ,Batch index: [1/10000], Loss Value:[0.55121058]\n",
            "Epoch: [1/200]  ,Batch index: [201/10000], Loss Value:[0.56104153]\n",
            "Epoch: [1/200]  ,Batch index: [401/10000], Loss Value:[0.51668715]\n",
            "Epoch: [1/200]  ,Batch index: [601/10000], Loss Value:[0.46920896]\n",
            "Epoch: [1/200]  ,Batch index: [801/10000], Loss Value:[0.51631397]\n",
            "Epoch: [1/200]  ,Batch index: [1001/10000], Loss Value:[0.46902987]\n",
            "Epoch: [1/200]  ,Batch index: [1201/10000], Loss Value:[0.46686172]\n",
            "Epoch: [1/200]  ,Batch index: [1401/10000], Loss Value:[0.47691041]\n",
            "Epoch: [1/200]  ,Batch index: [1601/10000], Loss Value:[0.50785959]\n",
            "Epoch: [1/200]  ,Batch index: [1801/10000], Loss Value:[0.44824705]\n",
            "Epoch: [1/200]  ,Batch index: [2001/10000], Loss Value:[0.35941172]\n",
            "Epoch: [1/200]  ,Batch index: [2201/10000], Loss Value:[0.47547746]\n",
            "Epoch: [1/200]  ,Batch index: [2401/10000], Loss Value:[0.39145350]\n",
            "Epoch: [1/200]  ,Batch index: [2601/10000], Loss Value:[0.41670471]\n",
            "Epoch: [1/200]  ,Batch index: [2801/10000], Loss Value:[0.46998897]\n",
            "Epoch: [1/200]  ,Batch index: [3001/10000], Loss Value:[0.42585620]\n",
            "Epoch: [1/200]  ,Batch index: [3201/10000], Loss Value:[0.48971489]\n",
            "Epoch: [1/200]  ,Batch index: [3401/10000], Loss Value:[0.42778981]\n",
            "Epoch: [1/200]  ,Batch index: [3601/10000], Loss Value:[0.41686618]\n",
            "Epoch: [1/200]  ,Batch index: [3801/10000], Loss Value:[0.38920978]\n",
            "Epoch: [1/200]  ,Batch index: [4001/10000], Loss Value:[0.40987140]\n",
            "Epoch: [1/200]  ,Batch index: [4201/10000], Loss Value:[0.38888416]\n",
            "Epoch: [1/200]  ,Batch index: [4401/10000], Loss Value:[0.41415498]\n",
            "Epoch: [1/200]  ,Batch index: [4601/10000], Loss Value:[0.39682975]\n",
            "Epoch: [1/200]  ,Batch index: [4801/10000], Loss Value:[0.39576373]\n",
            "Epoch: [1/200]  ,Batch index: [5001/10000], Loss Value:[0.34788090]\n",
            "Epoch: [1/200]  ,Batch index: [5201/10000], Loss Value:[0.35588807]\n",
            "Epoch: [1/200]  ,Batch index: [5401/10000], Loss Value:[0.44289151]\n",
            "Epoch: [1/200]  ,Batch index: [5601/10000], Loss Value:[0.39068347]\n",
            "Epoch: [1/200]  ,Batch index: [5801/10000], Loss Value:[0.39546660]\n",
            "Epoch: [1/200]  ,Batch index: [6001/10000], Loss Value:[0.33902925]\n",
            "Epoch: [1/200]  ,Batch index: [6201/10000], Loss Value:[0.36648974]\n",
            "Epoch: [1/200]  ,Batch index: [6401/10000], Loss Value:[0.40569237]\n",
            "Epoch: [1/200]  ,Batch index: [6601/10000], Loss Value:[0.36460823]\n",
            "Epoch: [1/200]  ,Batch index: [6801/10000], Loss Value:[0.38394758]\n",
            "Epoch: [1/200]  ,Batch index: [7001/10000], Loss Value:[0.40797505]\n",
            "Epoch: [1/200]  ,Batch index: [7201/10000], Loss Value:[0.39931533]\n",
            "Epoch: [1/200]  ,Batch index: [7401/10000], Loss Value:[0.33278137]\n",
            "Epoch: [1/200]  ,Batch index: [7601/10000], Loss Value:[0.37195224]\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "# --- Training loop based on number of epoch\n",
        "temp = 0.075\n",
        "for epoch in range(epoch_check,200):\n",
        "  print(80*'=')\n",
        "  \n",
        "  # --- For saving imformation\n",
        "  triplet_loss_sum = 0.0\n",
        "  len_face_data = len(face_data)\n",
        "\n",
        "  # -- set starting time\n",
        "  time0 = time.time()\n",
        "\n",
        "  # --- make learning rate update\n",
        "  if 50 < len(loss_arr):\n",
        "    for g in optimizer.param_groups:\n",
        "      g['lr'] = 0.001\n",
        "      temp = 0.001\n",
        "\n",
        "  # --- loop on batches\n",
        "  for batch_idx, batch_faces in enumerate(face_data):\n",
        "    # --- Extract face triplets and send them to CPU or GPU\n",
        "    anc_img = batch_faces['anc_img'].to(device)\n",
        "    pos_img = batch_faces['pos_img'].to(device)\n",
        "    neg_img = batch_faces['neg_img'].to(device)\n",
        "\n",
        "    # --- Get embedded values for each triplet\n",
        "    anc_embed = model(anc_img)\n",
        "    pos_embed = model(pos_img)\n",
        "    neg_embed = model(neg_img)\n",
        "    \n",
        "\n",
        "    # --- Find Distance\n",
        "    pos_dist = l2_dist.forward(anc_embed, pos_embed)\n",
        "    neg_dist = l2_dist.forward(anc_embed, neg_embed)\n",
        "\n",
        "    # --- Select hard triplets\n",
        "    all = (neg_dist - pos_dist < 0.8).cpu().numpy().flatten()\n",
        "    hard_triplets = np.where(all == 1)\n",
        "\n",
        "    if len(hard_triplets[0]) == 0: # --- Check number of hard triplets\n",
        "      continue\n",
        "    \n",
        "    # --- select hard embeds\n",
        "    anc_hard_embed = anc_embed[hard_triplets]\n",
        "    pos_hard_embed = pos_embed[hard_triplets]\n",
        "    neg_hard_embed = neg_embed[hard_triplets]\n",
        "\n",
        "    # --- Loss\n",
        "    loss_value = tiplet_loss.forward(anc_hard_embed, pos_hard_embed, neg_hard_embed)\n",
        "\n",
        "    # --- backward path\n",
        "    optimizer.zero_grad()\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (batch_idx % 200 == 0) : print(\"Epoch: [{}/{}]  ,Batch index: [{}/{}], Loss Value:[{:.8f}]\".format(epoch+1, epochs, batch_idx+1, len_face_data,loss_value))\n",
        "    # --- save information\n",
        "    triplet_loss_sum += loss_value.item()\n",
        "  \n",
        "  print(\"Learning Rate: \", temp)\n",
        "\n",
        "\n",
        "  # --- Find Avg. loss value\n",
        "  avg_triplet_loss = triplet_loss_sum / len_face_data\n",
        "  loss_arr.append(avg_triplet_loss)\n",
        "\n",
        "  \n",
        "  # --- Validation part besed on LFW Dataset\n",
        "  validation_acc = lfw_validation(model)\n",
        "  valid_arr.append(validation_acc)\n",
        "  model.train()\n",
        "\n",
        "  # --- Save model with checkpoints\n",
        "  save_model(model, avg_triplet_loss, epoch+1, optimizer, validation_acc, valid_arr, loss_arr)\n",
        "\n",
        "  # --- Print information for each epoch\n",
        "  print(\" Train set - Triplet Loss    =  {:.8f}\".format(avg_triplet_loss))\n",
        "  print(' Train set - Accuracy        = {:.8f}'.format(validation_acc))\n",
        "  print(f' Execution time              = {time.time() - time0}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgMMzRnC_lfy"
      },
      "source": [
        "# plot and print some information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "WY4wf27N_jck",
        "outputId": "d2488693-e6f1-4793-c3c2-6bbb51375eca"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plotvalid_arr(, 'b-', \n",
        "         label='Validation Accuracy',\n",
        "         \n",
        "         )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "4VZ2dfApB6Sc",
        "outputId": "d9f3e953-8430-4e44-df56-74bafbf4262e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(loss_arr, 'b-', \n",
        "         label='loss values',\n",
        "         \n",
        "         )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9x-BgOYeL-I",
        "outputId": "8c205bc9-3791-4452-ba1d-fbf0857ec7d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.01875\n"
          ]
        }
      ],
      "source": [
        "for param_group in optimizer.param_groups:\n",
        "    print(param_group['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC2amnc2eMuT"
      },
      "outputs": [],
      "source": [
        "valid_arr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V3SfBOeF1nSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7M56_nsC1nU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BvbY04eJ1nXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5SqHV4BM1nZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(40*\"=\" + \" Download CASIA WebFace \" + 40*'=')\n",
        "! gdown --id 1Of_EVz-yHV7QVWQGihYfvtny9Ne8qXVz\n",
        "! unzip CASIA-WebFace.zip\n",
        "! rm CASIA-WebFace.zip"
      ],
      "metadata": {
        "id": "dwopTQA11ncH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NvsyMJueMxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c27d78-989e-4738-db04-d5bec9945a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9349], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# --- LFW Dataset loading for test part\n",
        "l2_dist = PairwiseDistance(2)\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "valid_thresh = 0.96\n",
        "\n",
        "  \n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  # --- extracting\n",
        "  pair_one_path = \"./3.jpg\"\n",
        "  # print(pair_one_path)\n",
        "  pair_two_path = \"./2.jpg\"\n",
        "\n",
        "  # --- detect face and resize it\n",
        "  pair_one_img, flag_one = face_detect(pair_one_path)\n",
        "  pair_two_img, flag_two = face_detect(pair_two_path)\n",
        "\n",
        "  # --- Model Predict\n",
        "  pair_one_img = transform_list(pair_one_img[0])\n",
        "  pair_two_img = transform_list(pair_two_img[0])\n",
        "  pair_one_embed = model(torch.unsqueeze(pair_one_img, 0).to(device))\n",
        "  pair_two_embed = model(torch.unsqueeze(pair_two_img, 0).to(device))\n",
        "\n",
        "  # --- find Distance\n",
        "  pairs_dist = l2_dist.forward(pair_one_embed, pair_two_embed)\n",
        "  print(pairs_dist)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create Triplet Datasets ---\n",
        "# --- make a list of ids and folders\n",
        "selected_ids = np.uint32(np.round((np.random.rand(int(Triplet_size))) * (NumberID-1)))\n",
        "folders = os.listdir(\"./CASIA-WebFace/\")\n",
        "\n",
        "# --- Itrate on each id and make Triplets list\n",
        "TripletList = []\n",
        "\n",
        "for index,id in enumerate(selected_ids):\n",
        "  # --- print info\n",
        "  # print(40*\"=\" + str(index) + 40*\"=\")\n",
        "  # print(index)\n",
        "\n",
        "  # --- find name of id faces folder\n",
        "  id_str = str(folders[id])\n",
        "\n",
        "  # --- find list of faces in this folder\n",
        "  number_faces = os.listdir(\"./CASIA-WebFace/\"+id_str)\n",
        "\n",
        "  # --- Get two Random number for Anchor and Positive\n",
        "  while(True):\n",
        "    two_random = np.uint32(np.round(np.random.rand(2) * (len(number_faces)-1))) \n",
        "    if (two_random[0] != two_random[1]):\n",
        "      break\n",
        "\n",
        "  # --- Make Anchor and Positive image\n",
        "  Anchor   = str(number_faces[two_random[0]])\n",
        "  Positive = str(number_faces[two_random[1]])\n",
        "\n",
        "  # --- Make Negative image\n",
        "  while(True):\n",
        "    neg_id = np.uint32(np.round(np.random.rand(1) * (NumberID-1)))\n",
        "    if (neg_id != id):\n",
        "      break\n",
        "  # --- number of images in negative Folder\n",
        "  neg_id_str   = str(folders[neg_id[0]])\n",
        "  number_faces = os.listdir(\"./CASIA-WebFace/\"+neg_id_str)\n",
        "  one_random   = np.uint32(np.round(np.random.rand(1) * (len(number_faces)-1))) \n",
        "  Negative     = str(number_faces[one_random[0]])\n",
        "  \n",
        "  # --- insert Anchor, Positive and Negative image path to TripletList\n",
        "  TempList = [\"\",\"\",\"\"]\n",
        "  TempList[0] =  id_str + \"/\" + Anchor\n",
        "  TempList[1] =  id_str + \"/\" + Positive\n",
        "  TempList[2] =  neg_id_str + \"/\" + Negative\n",
        "  TripletList.append(TempList)\n",
        "  # print(TripletList[-1])"
      ],
      "metadata": {
        "id": "7SBeEfrnzB1F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5rvwhhKu09yf",
        "OczsCHs-1KlQ",
        "d87k5QG91Pxn",
        "v-pLtZni15ru",
        "MpIyf3wIVvrV"
      ],
      "machine_shape": "hm",
      "name": "ResNet+TripletLoss+CASIAFACE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}